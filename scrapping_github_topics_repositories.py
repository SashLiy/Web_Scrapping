# -*- coding: utf-8 -*-
"""scrapping-github-topics-repositories.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWm1AmwD2Y8A_ymbFAuqPEp40J7Svg3l

# Top Repositories for GitHub Topics

Use the "Run" button to execute the code.

### Pick a website and describe your objective
- Browse through different sites and pick on to scrape. Check the "Project Ideas" section for inspiration.
- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.
- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the "New" button above.

Outline :
- we are going to scrape https://github.com/topics
- We will get all the topics, for each topic, topic, title, topic url and topic description
- For each topic, we will get top 25 repositories in the topic from the topic page.
- For each repositories, grab the repo name, username and repo url
- For each topic, create a csv file.

### Use the requests library to download web pages
- Inspect the website's HTML source and identify the right URLs to download.
- Download and save web pages locally using the requests library.
- Create a function to automate downloading for different topics/search queries.
"""

!pip install requests --upgrade --quiet

import requests

# Defining the URL for GitHub topics page
topics_url= "https://github.com/topics"

# Getting the HTML content of the webpage
response = requests.get(topics_url)

response.status_code
# the code checks if the status code is 200, indicating the request was successful. If it is not, an exception is raised to indicate that the page failed to load.

len(response.text)

# Extracting the HTML content of the webpage as a string
Page_contents = response.text

# Displaying the first 1000 characters of the HTML content
# This is useful for inspecting a snippet of the HTML structure to understand its format and contents
page_contents[:1000]

# Saving the HTML content to a file
# This writes the HTML content of the webpage to a file named 'webpage.html'
# It allows for offline inspection and debugging of the webpage content

with open('webpage.html','w')as f:
  f.write(page_contents)

"""### Use Beautiful Soup to parse and extract information

- Parse and explore the structure of downloaded web pages using Beautiful soup.
- Use the right properties and methods to extract the required information.
- Create functions to extract from the page into lists and dictionaries.
- (Optional) Use a REST API to acquire additional information if required.
"""

!pip install beautifulsoup4 --upgrade --quiet

from bs4 import BeautifulSoup

# Parse the HTML content of the page using BeautifulSoup
# 'page_contents' is the variable containing the HTML content
# 'html.parser' specifies the parser to use for parsing the HTML
doc = BeautifulSoup(page_contents, 'html.parser')

doc

type(doc)

# Define the class name to search for in the HTML content
selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'

# Find all <p> tags in the parsed HTML document that have the specified class
# 'doc' is the BeautifulSoup object containing the parsed HTML
# 'find_all' returns a list of all matching <p> tags
topic_title_tags = doc.find_all('p', {'class': selection_class})

len(topic_title_tags)

topic_title_tags[:5]

topic_title_tags

# Define the class name to search for in the HTML content for descriptions
desc_selector = "f5 color-fg-muted mb-0 mt-1"

# Find all <p> tags in the parsed HTML document that have the specified class
# 'doc' is the BeautifulSoup object containing the parsed HTML
# 'find_all' returns a list of all matching <p> tags with the class 'desc_selector'
topic_desc_tags = doc.find_all('p',{'class':desc_selector })

topic_desc_tags

topic_title_tag0 = topic_title_tags[0]

topic_title_tag0

div_tags = topic_title_tag0.parent

div_tags

# Define the class name to search for in the HTML content for topic links
# This class likely represents anchor (<a>) tags used for topic links
topic_links_tags = doc.find_all('a', {'class':"no-underline flex-1 d-flex flex-column"})

topic_links_tags[0]

# Construct the full URL for the first topic link
# 'topic_links_tags' is a list of <a> tags, and [0] accesses the first tag
# 'href' is the attribute of the <a> tag that contains the relative URL
# 'https://github.com' is the base URL to which the relative URL is appended
topic0_url = "https://github.com" + topic_links_tags[0]['href']


# Print the constructed full URL to the console
print(topic0_url)

topic_title_tags[0].text

topic_titles = []
for tag in topic_title_tags:
  topic_titles.append(tag.text)
print(topic_titles)

topic_descriptions= []
for tag in topic_desc_tags:
  topic_descriptions.append(tag.text.strip())
  print(topic_descriptions)

topic_urls =[]
base_url = "https://github.com"

for tag in topic_links_tags:
  topic_urls.append(base_url+ tag['href'])

topic_urls

!pip install pandas --quiet

import pandas as pd

# Create a dictionary to store topic-related information
# 'title' will hold the list of topic titles
# 'description' will hold the list of topic descriptions
# 'url' will hold the list of topic URLs

topics_dict = {
    'title': topic_titles,
    'description': topic_descriptions,
    'url': topic_urls
}

topics_df = pd.DataFrame(topics_dict)

topics_df

"""### Create CSV file(s) with the extracted information

- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.
- Execute the function with different inputs to create a dataset of CSV files.
- Verify the information in the CSV files by reading them back using Pandas.

"""

topics_df.to_csv('topics.csv', index= None)

"""### Getting info out of Topic page
- Find more information of 3D Topic page
"""

topic_page_url = topic_urls[0]

topic_page_url

response = requests.get(topic_page_url)

response.status_code

topic_doc = BeautifulSoup(response.text, 'html.parser')

# Define the class name to search for in the HTML content for repository headings
h3_selection_class = "f3 color-fg-muted text-normal lh-condensed"

# Find all <h3> tags in the parsed HTML document that have the specified class
# 'topic_doc' is the BeautifulSoup object containing the parsed HTML
# 'find_all' returns a list of all matching <h3> tags with the specified class
repo_tags = topic_doc.find_all('h3',{'class': h3_selection_class })

repo_tags

len(repo_tags)

# Find all <a> tags within the first <h3> tag in the 'repo_tags' list
# 'repo_tags' is a list of <h3> tags, and [0] accesses the first <h3> tag
# 'find_all' retrieves all <a> tags within that <h3> tag
a_tags = repo_tags[0].find_all('a')

a_tags[0].text.strip()

a_tags[1].text.strip()

base_url = "https://github.com"
repo_url = base_url+ a_tags[1]['href']
print(repo_url)

"""### Define Functions"""

def get_repo_infor(h3_tag):
  a_tags = h3_tag.find_all('a')
  username = a_tags[0].text.strip()
  repo_name = a_tags[1].text.strip()
  repo_url = base_url + a_tags[1]['href']
  return username, repo_name, repo_url

get_repo_infor(repo_tags[0])

import jovian

jovian.commit()

"""### Image extract"""

r = requests.get("https://repository-images.githubusercontent.com/11007313/03147e80-4379-11ea-80b8-58c66da0bf29")

r.status_code

with open('image.jpg','wb') as f:
  f.write(r.content)

"""### Document and share your work

- Add proper headings and documentation in your Jupyter notebook.
- Publish your Jupyter notebook to your Jovian profile
- (Optional) Write a blog post about your project and share it online.
"""